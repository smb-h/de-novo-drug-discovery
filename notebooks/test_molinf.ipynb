{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 21:22:53.704715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 21:22:53.858719: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "\n",
    "from rdkit.Chem import MolStandardize, MolFromSmiles, MolToSmiles\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c1ccc(cc1)[N-]S(=O)(=O)CCCCCCC(=O)NO',\n",
       " 'c1cc(ccc1[N+](=O)[O-])OC[C@H]2CO2',\n",
       " 'c1cc(ccc1[N+](=O)[O-])OC[C@@H]2CO2',\n",
       " 'c1cc(c(cc1[N+](=O)[O-])[N+](=O)[O-])Cl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/raw/ZINC_results.smi\") as f:\n",
    "    smiles = [s.split(\"\\t\")[0].rstrip() for s in f]\n",
    "smiles[:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesTokenizer(object):\n",
    "    def __init__(self):\n",
    "        atoms = [\n",
    "            \"Al\",\n",
    "            \"As\",\n",
    "            \"B\",\n",
    "            \"Br\",\n",
    "            \"C\",\n",
    "            \"Cl\",\n",
    "            \"F\",\n",
    "            \"H\",\n",
    "            \"I\",\n",
    "            \"K\",\n",
    "            \"Li\",\n",
    "            \"N\",\n",
    "            \"Na\",\n",
    "            \"O\",\n",
    "            \"P\",\n",
    "            \"S\",\n",
    "            \"Se\",\n",
    "            \"Si\",\n",
    "            \"Te\",\n",
    "        ]\n",
    "        special = [\n",
    "            \"(\",\n",
    "            \")\",\n",
    "            \"[\",\n",
    "            \"]\",\n",
    "            \"=\",\n",
    "            \"#\",\n",
    "            \"%\",\n",
    "            \"0\",\n",
    "            \"1\",\n",
    "            \"2\",\n",
    "            \"3\",\n",
    "            \"4\",\n",
    "            \"5\",\n",
    "            \"6\",\n",
    "            \"7\",\n",
    "            \"8\",\n",
    "            \"9\",\n",
    "            \"+\",\n",
    "            \"-\",\n",
    "            \"se\",\n",
    "            \"te\",\n",
    "            \"c\",\n",
    "            \"n\",\n",
    "            \"o\",\n",
    "            \"s\",\n",
    "        ]\n",
    "        padding = [\"G\", \"A\", \"E\"]\n",
    "\n",
    "        self.table = sorted(atoms, key=len, reverse=True) + special + padding\n",
    "        table_len = len(self.table)\n",
    "\n",
    "        self.table_2_chars = list(filter(lambda x: len(x) == 2, self.table))\n",
    "        self.table_1_chars = list(filter(lambda x: len(x) == 1, self.table))\n",
    "\n",
    "        self.one_hot_dict = {}\n",
    "        for i, symbol in enumerate(self.table):\n",
    "            vec = np.zeros(table_len, dtype=np.float32)\n",
    "            vec[i] = 1\n",
    "            self.one_hot_dict[symbol] = vec\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "        smiles = smiles + \" \"\n",
    "        N = len(smiles)\n",
    "        token = []\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            c1 = smiles[i]\n",
    "            c2 = smiles[i : i + 2]\n",
    "\n",
    "            if c2 in self.table_2_chars:\n",
    "                token.append(c2)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            if c1 in self.table_1_chars:\n",
    "                token.append(c1)\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return token\n",
    "\n",
    "    def one_hot_encode(self, tokenized_smiles):\n",
    "        result = np.array(\n",
    "            [self.one_hot_dict[symbol] for symbol in tokenized_smiles], dtype=np.float32\n",
    "        )\n",
    "        result = result.reshape(1, result.shape[0], result.shape[1])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.normarizer = MolStandardize.normalize.Normalizer()\n",
    "        self.lfc = MolStandardize.fragment.LargestFragmentChooser()\n",
    "        self.uc = MolStandardize.charge.Uncharger()\n",
    "\n",
    "    def process(self, smi):\n",
    "        mol = MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            mol = self.normarizer.normalize(mol)\n",
    "            mol = self.lfc.choose(mol)\n",
    "            mol = self.uc.uncharge(mol)\n",
    "            smi = MolToSmiles(mol, isomericSmiles=False, canonical=True)\n",
    "            return smi\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input SMILES num: 5000\n",
      "start preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:05<00:00, 928.05it/s] \n",
      "100%|██████████| 4998/4998 [00:00<00:00, 47463.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  0\n",
      "Error Tokens:  []\n",
      "output SMILES num: 4332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessor()\n",
    "\n",
    "print(f\"input SMILES num: {len(smiles)}\")\n",
    "print(\"start preprocessing...\")\n",
    "\n",
    "smiles = [pp.process(smi) for smi in tqdm(smiles)]\n",
    "smiles = list(set([s for s in smiles if s]))\n",
    "\n",
    "# token limits (34 to 74)\n",
    "st = SmilesTokenizer()\n",
    "smiles_tokenized = [st.tokenize(smi) for smi in tqdm(smiles)]\n",
    "smiles_processed = []\n",
    "smiles_max_len = 0\n",
    "\n",
    "\n",
    "# check for not recognized tokens\n",
    "err = 0\n",
    "err_tokens = []\n",
    "for i in range(len(smiles)):\n",
    "    if smiles[i] != \"\".join(smiles_tokenized[i]):\n",
    "        print(\"=====================================\")\n",
    "        print(len(smiles[i]), \" :\", smiles[i])\n",
    "        print(len(smiles_tokenized[i]), \" :\" ,smiles_tokenized[i])\n",
    "        for char in smiles[i]:\n",
    "            if char not in smiles_tokenized[i]:\n",
    "                err_tokens.append(char)\n",
    "        err += 1\n",
    "print(\"Error: \", err)\n",
    "print(\"Error Tokens: \", err_tokens)\n",
    "\n",
    "\n",
    "for tokenized in smiles_tokenized:\n",
    "    if 34 <= len(tokenized) <= 74:\n",
    "        smiles_processed.append(tokenized)\n",
    "        # update smiles max len\n",
    "        if len(tokenized) > smiles_max_len:\n",
    "            smiles_max_len = len(tokenized)\n",
    "\n",
    "print(f\"output SMILES num: {len(smiles_processed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad(tokenized_smi):\n",
    "    return (\n",
    "        [\"G\"] + tokenized_smi + [\"E\"] + [\"A\" for _ in range(smiles_max_len - len(tokenized_smi))]\n",
    "    )\n",
    "\n",
    "def _padding(data):\n",
    "    padded_smiles = [_pad(t_smi) for t_smi in data]\n",
    "    return padded_smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN1CC2CCC1CN(C(=O)Nc1ccc(Cl)cc1Cl)C2\n",
      "GCN1CC2CCC1CN(C(=O)Nc1ccc(Cl)cc1Cl)C2EAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
     ]
    }
   ],
   "source": [
    "# add paddings\n",
    "print(\"\".join(smiles_processed[0]))\n",
    "smiles_processed = _padding(smiles_processed)\n",
    "print(\"\".join(smiles_processed[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "GCN1CC2CCC1CN(C(=O)Nc1ccc(Cl)cc1Cl)C2EAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "CN1CC2CCC1CN(C(=O)Nc1ccc(Cl)cc1Cl)C2EAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "-----------------------------------\n",
      "GCCn1ccc2ccc(CN3CC4CCC3CN(S(=O)(=O)N(C)C)C4)cc21EAAAAAAAAAAAAAAA\n",
      "CCn1ccc2ccc(CN3CC4CCC3CN(S(=O)(=O)N(C)C)C4)cc21EAAAAAAAAAAAAAAAA\n",
      "-----------------------------------\n",
      "GCCS(=O)(=O)N1CC2CCC1CN(C(=O)c1ccncc1)C2EAAAAAAAAAAAAAAAAAAAAAAA\n",
      "CCS(=O)(=O)N1CC2CCC1CN(C(=O)c1ccncc1)C2EAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "-----------------------------------\n",
      "GO=C(c1cccnc1-c1ccc(OC(F)(F)F)cc1)N1CC2CCC1C2EAAAAAAAAAAAAAAAAAA\n",
      "O=C(c1cccnc1-c1ccc(OC(F)(F)F)cc1)N1CC2CCC1C2EAAAAAAAAAAAAAAAAAAA\n",
      "-----------------------------------\n",
      "GCc1cc(CN2CC3CCC2CN(C2Cc4ccccc4C2)C3)on1EAAAAAAAAAAAAAAAAAAAAAAA\n",
      "Cc1cc(CN2CC3CCC2CN(C2Cc4ccccc4C2)C3)on1EAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "(4332, 64, 47)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode\n",
    "x, y = [], []\n",
    "\n",
    "for tp_smi in smiles_processed[:5]:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"\".join(tp_smi[:-1]))\n",
    "    print(\"\".join(tp_smi[1:]))\n",
    "\n",
    "for tp_smi in smiles_processed:\n",
    "    _x = [st.one_hot_dict[symbol] for symbol in tp_smi[:-1]]\n",
    "    x.append(_x)\n",
    "    _y = [st.one_hot_dict[symbol] for symbol in tp_smi[1:]]\n",
    "    y.append(_y)\n",
    "\n",
    "x = np.array(x, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "print(x.shape)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 20% for validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x.shape[0]):\n",
    "    if x[i].shape != y[i].shape:\n",
    "        print(x[i].shape)\n",
    "        print(y[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 21:23:02.267064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_EX3 (InputLayer)         [(None, 64, 47)]     0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64, 60)       2880        ['Input_EX3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64, 60)       3660        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " Input_Ex1 (InputLayer)         [(None, 64, 47)]     0           []                               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64, 60)       3660        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " polarizer (InputLayer)         [(None, 64, 47)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 64, 60)       25920       ['Input_Ex1[0][0]']              \n",
      "                                                                                                  \n",
      " customized_layer__polarizer (C  [(None, 64, 60),    0           ['dense_2[0][0]']                \n",
      " ustomizedLayer_Polarizer)       (None, 64, 60)]                                                  \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 64, 60)       25920       ['polarizer[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64, 120)      0           ['lstm[0][0]',                   \n",
      "                                                                  'customized_layer__polarizer[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 120)      0           ['lstm_1[0][0]',                 \n",
      "                                                                  'customized_layer__polarizer[0][\n",
      "                                                                 1]']                             \n",
      "                                                                                                  \n",
      " customized_layer__attention (C  (None, 64, 60)      0           ['concatenate[0][0]']            \n",
      " ustomizedLayer_Attention)                                                                        \n",
      "                                                                                                  \n",
      " customized_layer__attention_1   (None, 64, 60)      0           ['concatenate_1[0][0]']          \n",
      " (CustomizedLayer_Attention)                                                                      \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 64, 60)       29040       ['customized_layer__attention[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 64, 60)       29040       ['customized_layer__attention_1[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64, 35)       2135        ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64, 35)       2135        ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 64, 70)       0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 70)      280         ['concatenate_2[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_variational (DenseVariat  (None, 64, 70)      9940        ['batch_normalization[0][0]']    \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " dense_variational_1 (DenseVari  (None, 64, 70)      9940        ['dense_variational[0][0]']      \n",
      " ational)                                                                                         \n",
      "                                                                                                  \n",
      " dense_variational_2 (DenseVari  (None, 64, 70)      9940        ['dense_variational_1[0][0]']    \n",
      " ational)                                                                                         \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64, 47)       3337        ['dense_variational_2[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 157,827\n",
      "Trainable params: 157,687\n",
      "Non-trainable params: 140\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class CustomizedLayer_Polarizer(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomizedLayer_Polarizer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        G_thesis = inputs\n",
    "        G_antithesis = 1 - inputs\n",
    "\n",
    "        return [G_thesis, G_antithesis]\n",
    "\n",
    "\n",
    "class CustomizedLayer_Attention(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomizedLayer_Attention, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # G_LSTM= inputs[:,:60]\n",
    "        # G_Attention= inputs[:,60:]\n",
    "        # res= tf.math.add(G_LSTM, G_Attention)\n",
    "        elem_prod = inputs[:, :, :60] + inputs[:, :, 60:]\n",
    "        # res = k.sum(elem_prod, axis=-1, keepdims=True)\n",
    "        return elem_prod\n",
    "\n",
    "\n",
    "def prior_mean_field(kernel_size, bias_size, dtype=None):  # prior Function\n",
    "    n = kernel_size + bias_size\n",
    "    return lambda t: tfd.Independent(\n",
    "        tfd.Normal(loc=tf.zeros(n, dtype=dtype), scale=tf.ones(n)), reinterpreted_batch_ndims=1\n",
    "    )\n",
    "\n",
    "\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):  # Posterior Function\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.0))\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "            tfp.layers.DistributionLambda(\n",
    "                lambda t: tfd.Independent(\n",
    "                    tfd.Normal(loc=t[..., :n], scale=1e-5 + 0.01 * tf.nn.softplus(c + t[..., n:])),\n",
    "                    reinterpreted_batch_ndims=1,\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "data_len = x.shape[0]\n",
    "hidden_units = [70, 70, 70]\n",
    "batch_size = 50\n",
    "counter_L = 0\n",
    "look_back = 1\n",
    "model = Sequential()\n",
    "# InData_Ex1 = layers.Input(shape=([1, 70]), name=\"Input_Ex1\")\n",
    "# InData_Ex2 = layers.Input(shape=([1, 70]), name=\"polarizer\")\n",
    "# InData_Ex3 = layers.Input(shape=([1, 70]), name=\"Input_EX3\")\n",
    "InData_Ex1 = layers.Input(shape=([64, 47]), name=\"Input_Ex1\")\n",
    "InData_Ex2 = layers.Input(shape=([64, 47]), name=\"polarizer\")\n",
    "InData_Ex3 = layers.Input(shape=([64, 47]), name=\"Input_EX3\")\n",
    "EX_lstm1 = layers.LSTM(60, return_sequences=True)(InData_Ex1)\n",
    "EX_lstm2 = layers.LSTM(60, return_sequences=True)(InData_Ex2)\n",
    "GateIn = layers.Dense(units=60, activation=\"sigmoid\")(InData_Ex3)\n",
    "Gate_pp = layers.Dense(units=60, activation=\"sigmoid\")(GateIn)\n",
    "Gate_pp = layers.Dense(units=60, activation=\"sigmoid\")(Gate_pp)\n",
    "CFPG = CustomizedLayer_Polarizer(units=60)(Gate_pp)\n",
    "# GatesODD = layers.Dense(units=60, activation='sigmoid')(GatesIn)\n",
    "MultiplictionEven_In = layers.Concatenate(axis=-1)([EX_lstm1, CFPG[0]])\n",
    "MultiplictionEven_In = CustomizedLayer_Attention()(MultiplictionEven_In)\n",
    "EX_lstm1 = layers.LSTM(60, return_sequences=True)(MultiplictionEven_In)\n",
    "MultiplictionEven = layers.Dense(units=35, activation=\"sigmoid\")(EX_lstm1)\n",
    "MultiplictionODD_In = layers.Concatenate(axis=-1)([EX_lstm2, CFPG[1]])\n",
    "MultiplictionODD_In = CustomizedLayer_Attention()(MultiplictionODD_In)\n",
    "EX_lstm2 = layers.LSTM(60, return_sequences=True)(MultiplictionODD_In)\n",
    "MultiplictionODD = layers.Dense(units=35, activation=\"sigmoid\")(EX_lstm2)\n",
    "# features = layers.Concatenate([InData_Ex1, InData_Ex2, InData_Ex3, InData_Ex4])\n",
    "InData = layers.Concatenate(axis=-1)([MultiplictionEven, MultiplictionODD])\n",
    "InData = layers.BatchNormalization()(InData)\n",
    "features = InData\n",
    "for units in hidden_units:\n",
    "    features = tfp.layers.DenseVariational(\n",
    "        units=units,\n",
    "        make_prior_fn=prior_mean_field,\n",
    "        make_posterior_fn=posterior_mean_field,\n",
    "        kl_weight=1 / data_len,\n",
    "        activation=\"relu\",\n",
    "    )(features)\n",
    "# features = layers.Dense(units=70, activation=\"sigmoid\")(features)\n",
    "features = layers.Dense(units=47, activation=\"sigmoid\")(features)\n",
    "model = keras.Model(inputs=[InData_Ex1, InData_Ex2, InData_Ex3], outputs=features)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(\n",
    "            \"../reports/\",\n",
    "            \"2022-11-15\",\n",
    "            \"test\",\n",
    "            \"checkpoints\",\n",
    "            '{epoch:02d}.hdf5'),\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=True,\n",
    "    )\n",
    ")\n",
    "# create checkpoints dir\n",
    "os.makedirs(os.path.join(\n",
    "    \"../reports/\",\n",
    "    \"2022-11-15\",\n",
    "    \"test\",\n",
    "    \"checkpoints\"), exist_ok=True)\n",
    "\n",
    "callbacks.append(\n",
    "    TensorBoard(\n",
    "        log_dir=os.path.join(\n",
    "            \"../reports/\",\n",
    "            \"2022-11-15\",\n",
    "            \"test\",\n",
    "            \"logs\",\n",
    "        ),\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq=\"epoch\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Layer CustomizedLayer_Polarizer has arguments ['units']\n",
      "in `__init__` and therefore must override `get_config()`.\n",
      "\n",
      "Example:\n",
      "\n",
      "class CustomLayer(keras.layers.Layer):\n",
      "    def __init__(self, arg1, arg2):\n",
      "        super().__init__()\n",
      "        self.arg1 = arg1\n",
      "        self.arg2 = arg2\n",
      "\n",
      "    def get_config(self):\n",
      "        config = super().get_config()\n",
      "        config.update({\n",
      "            \"arg1\": self.arg1,\n",
      "            \"arg2\": self.arg2,\n",
      "        })\n",
      "        return config\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.8754 - accuracy: 0.7227\n",
      "Epoch 1: saving model to ../reports/2022-11-15/test/checkpoints/01.hdf5\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 8.8754 - accuracy: 0.7227\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.8133 - accuracy: 0.7272\n",
      "Epoch 2: saving model to ../reports/2022-11-15/test/checkpoints/02.hdf5\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 8.8133 - accuracy: 0.7272\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.7539 - accuracy: 0.7343\n",
      "Epoch 3: saving model to ../reports/2022-11-15/test/checkpoints/03.hdf5\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 8.7539 - accuracy: 0.7343\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - ETA: 0s - loss: 8.7001 - accuracy: 0.7364\n",
      "Epoch 4: saving model to ../reports/2022-11-15/test/checkpoints/04.hdf5\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 8.7001 - accuracy: 0.7364\n",
      "Epoch 5/5\n",
      " 95/100 [===========================>..] - ETA: 0s - loss: 8.6425 - accuracy: 0.7390WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 500 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 5: saving model to ../reports/2022-11-15/test/checkpoints/05.hdf5\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 8.6425 - accuracy: 0.7390\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "            {\"Input_Ex1\": x_train, \"polarizer\": x_train, \"Input_EX3\": x_train},\n",
    "            y_train,\n",
    "            # steps_per_epoch=x_train.shape[0],\n",
    "            steps_per_epoch=100,\n",
    "            epochs=5,\n",
    "            verbose=True,\n",
    "            validation_data=({\"Input_Ex1\": x_val, \"polarizer\": x_val, \"Input_EX3\": x_val}, y_val),\n",
    "            # validation_steps=x_val.shape[0],\n",
    "            validation_steps=30,\n",
    "            use_multiprocessing=True,\n",
    "            shuffle=True,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "# last_weight_file = glob(\n",
    "#     os.path.join(\n",
    "#         \"../reports/\",\n",
    "#         \"2022-11-15\",\n",
    "#         \"test\",\n",
    "#         \"checkpoints\",\n",
    "#         'test-{30:02}*.hdf5')\n",
    "# )[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('drug_design')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df575d9b3e9895d2442e3df5b270c02c77977dbbc9d135026610c4079aa2b2a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
